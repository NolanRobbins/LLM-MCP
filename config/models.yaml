# Model configurations and capabilities
models:
  gpt-4-turbo:
    provider: openai
    context_length: 128000
    supports_functions: true
    supports_vision: true
    supports_streaming: true
    avg_latency_ms: 800
    cost_per_1k_tokens: 0.03
    quality_score: 0.95
    specialties: ["reasoning", "creative", "analysis"]
    
  gpt-3.5-turbo:
    provider: openai
    context_length: 16000
    supports_functions: true
    supports_vision: false
    supports_streaming: true
    avg_latency_ms: 400
    cost_per_1k_tokens: 0.0015
    quality_score: 0.85
    specialties: ["code", "general"]
    
  claude-3-opus:
    provider: anthropic
    context_length: 200000
    supports_functions: true
    supports_vision: true
    supports_streaming: true
    avg_latency_ms: 900
    cost_per_1k_tokens: 0.03
    quality_score: 0.96
    specialties: ["code", "reasoning", "long-form"]
    
  claude-3-sonnet:
    provider: anthropic
    context_length: 200000
    supports_functions: true
    supports_vision: true
    supports_streaming: true
    avg_latency_ms: 600
    cost_per_1k_tokens: 0.015
    quality_score: 0.92
    specialties: ["code", "reasoning"]
    
  gemini-pro:
    provider: google
    context_length: 32000
    supports_functions: true
    supports_vision: true
    supports_streaming: true
    avg_latency_ms: 600
    cost_per_1k_tokens: 0.001
    quality_score: 0.88
    specialties: ["multimodal", "reasoning"]
    
  mistral-medium:
    provider: mistral
    context_length: 32000
    supports_functions: true
    supports_vision: false
    supports_streaming: true
    avg_latency_ms: 400
    cost_per_1k_tokens: 0.0027
    quality_score: 0.85
    specialties: ["code", "multilingual"]
    
  mixtral-8x7b:
    provider: groq
    context_length: 32000
    supports_functions: false
    supports_vision: false
    supports_streaming: true
    avg_latency_ms: 200
    cost_per_1k_tokens: 0.0005
    quality_score: 0.82
    specialties: ["fast-inference", "code"]
